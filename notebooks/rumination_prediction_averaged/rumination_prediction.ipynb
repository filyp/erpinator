{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rumination prediction - averaged participants' epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_order_list = [\n",
    "    \"Fp1\",\n",
    "    \"AF7\",\n",
    "    \"AF3\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    \"F5\",\n",
    "    \"F7\",\n",
    "    \"FT7\",\n",
    "    \"FC5\",\n",
    "    \"FC3\",\n",
    "    \"FC1\",\n",
    "    \"C1\",\n",
    "    \"C3\",\n",
    "    \"C5\",\n",
    "    \"T7\",\n",
    "    \"TP7\",\n",
    "    \"CP5\",\n",
    "    \"CP3\",\n",
    "    \"CP1\",\n",
    "    \"P1\",\n",
    "    \"P3\",\n",
    "    \"P5\",\n",
    "    \"P7\",\n",
    "    \"P9\",\n",
    "    \"PO7\",\n",
    "    \"PO3\",\n",
    "    \"O1\",\n",
    "    \"Iz\",\n",
    "    \"Oz\",\n",
    "    \"POz\",\n",
    "    \"Pz\",\n",
    "    \"CPz\",\n",
    "    \"Fpz\",\n",
    "    \"Fp2\",\n",
    "    \"AF8\",\n",
    "    \"AF4\",\n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"F4\",\n",
    "    \"F6\",\n",
    "    \"F8\",\n",
    "    \"FT8\",\n",
    "    \"FC6\",\n",
    "    \"FC4\",\n",
    "    \"FC2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"C4\",\n",
    "    \"C6\",\n",
    "    \"T8\",\n",
    "    \"TP8\",\n",
    "    \"CP6\",\n",
    "    \"CP4\",\n",
    "    \"CP2\",\n",
    "    \"P2\",\n",
    "    \"P4\",\n",
    "    \"P6\",\n",
    "    \"P8\",\n",
    "    \"P10\",\n",
    "    \"PO8\",\n",
    "    \"PO4\",\n",
    "    \"O2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_dict = dict(zip(channels_order_list, np.arange(1, 64, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define significant channels - the rest will be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_box = [\n",
    "    \"F1\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"FC1\",\n",
    "    \"FCz\",\n",
    "    \"FC2\",\n",
    "    \"C1\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"CP1\",\n",
    "    \"CPz\",\n",
    "    \"CP2\",\n",
    "    \"P1\",\n",
    "    \"Pz\",\n",
    "    \"P2\",\n",
    "]\n",
    "significant_channels = [channels_dict[channel] for channel in red_box]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df_mean\"\n",
    "pickled_data_filename = \"../../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average participants' error and correct epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_epochs_df = (\n",
    "    epochs_df.groupby(\n",
    "        [\"id\", \"marker\"],\n",
    "        sort=False,\n",
    "    )\n",
    "    .apply(\n",
    "        lambda group_df: pd.Series(\n",
    "            {\n",
    "                \"epoch\": np.mean(group_df[\"epoch\"]),\n",
    "                \"Rumination Full Scale\": np.mean(group_df[\"Rumination Full Scale\"]),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "## Training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard features for EEG analysis provided by Guo et al. (2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_signal(t, m, e):\n",
    "    return np.std(m)\n",
    "\n",
    "\n",
    "def abs_diffs_signal(t, m, e):\n",
    "    return np.sum(np.abs(np.diff(m)))\n",
    "\n",
    "\n",
    "def mean_energy_signal(t, m, e):\n",
    "    return np.mean(m ** 2)\n",
    "\n",
    "\n",
    "def skew_signal(t, m, e):\n",
    "    return scipy.stats.skew(m)\n",
    "\n",
    "\n",
    "def mean_signal(t, m, e):\n",
    "    return np.mean(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guo_features = {\n",
    "    \"mean\": mean_signal,\n",
    "    \"std\": std_signal,\n",
    "    \"mean_energy\": mean_energy_signal,\n",
    "}\n",
    "\n",
    "step_in_ms = 50\n",
    "step_tp = int(signal_frequency * step_in_ms / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate p-value with permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "\n",
    "def calculate_p_permutations(estimator, X, y, cv=3, n_permutations=100, n_jobs=10):\n",
    "\n",
    "    score_, perm_scores_, pvalue_ = permutation_test_score(\n",
    "        estimator, X, y, cv=cv, n_permutations=n_permutations, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # summarize\n",
    "    print(f\"     The permutation P-value is = {pvalue_:.3f}\")\n",
    "    print(f\"     The permutation score is = {score_:.3f}\\n\")\n",
    "\n",
    "    return score_, pvalue_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation curves - for parameters' insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pooled_var(stds):\n",
    "    # https://en.wikipedia.org/wiki/Pooled_variance#Pooled_standard_deviation\n",
    "    n = 5  # size of each group\n",
    "    return np.sqrt(sum((n - 1) * (stds ** 2)) / len(stds) * (n - 1))\n",
    "\n",
    "\n",
    "def show_validation_curves(cv_results, grid_params):\n",
    "\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    results = [\n",
    "        \"mean_test_r2\",\n",
    "        \"mean_train_r2\",\n",
    "        \"std_test_r2\",\n",
    "        \"std_train_r2\",\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1, len(grid_params), figsize=(5 * len(grid_params), 7), sharey=\"row\"\n",
    "    )\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=25)\n",
    "\n",
    "    for idx, (param_name, param_range) in enumerate(grid_params.items()):\n",
    "        grouped_df = df.groupby(f\"param_{param_name}\")[results].agg(\n",
    "            {\n",
    "                \"mean_train_r2\": \"mean\",\n",
    "                \"mean_test_r2\": \"mean\",\n",
    "                \"std_train_r2\": pooled_var,\n",
    "                \"std_test_r2\": pooled_var,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        previous_group = df.groupby(f\"param_{param_name}\")[results]\n",
    "        axes[idx].set_xlabel(param_name, fontsize=10)\n",
    "        axes[idx].set_ylim(0.0, 1.1)\n",
    "        axes[idx].set_xscale(\"log\")\n",
    "        lw = 2\n",
    "        axes[idx].plot(\n",
    "            param_range,\n",
    "            grouped_df[\"mean_train_r2\"],\n",
    "            label=\"Training score\",\n",
    "            color=\"darkorange\",\n",
    "            lw=lw,\n",
    "        )\n",
    "        axes[idx].fill_between(\n",
    "            param_range,\n",
    "            grouped_df[\"mean_train_r2\"] - grouped_df[\"std_train_r2\"],\n",
    "            grouped_df[\"mean_train_r2\"] + grouped_df[\"std_train_r2\"],\n",
    "            alpha=0.2,\n",
    "            color=\"darkorange\",\n",
    "            lw=lw,\n",
    "        )\n",
    "        axes[idx].plot(\n",
    "            param_range,\n",
    "            grouped_df[\"mean_test_r2\"],\n",
    "            label=\"Cross-validation score\",\n",
    "            color=\"navy\",\n",
    "            lw=lw,\n",
    "        )\n",
    "        axes[idx].fill_between(\n",
    "            param_range,\n",
    "            grouped_df[\"mean_test_r2\"] - grouped_df[\"std_test_r2\"],\n",
    "            grouped_df[\"mean_test_r2\"] + grouped_df[\"std_test_r2\"],\n",
    "            alpha=0.2,\n",
    "            color=\"navy\",\n",
    "            lw=lw,\n",
    "        )\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle(\"Validation curves\", fontsize=40)\n",
    "    fig.legend(handles, labels, loc=8, ncol=2, fontsize=20)\n",
    "\n",
    "    fig.subplots_adjust(bottom=0.25, top=0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERROR\n",
    "dataset_name = \"correct\" if dataset == CORRECT else \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(\n",
    "    averaged_epochs_df[averaged_epochs_df[\"marker\"] == dataset][\"epoch\"].to_list()\n",
    ")\n",
    "y_train = np.array(\n",
    "    averaged_epochs_df[averaged_epochs_df[\"marker\"] == dataset][\n",
    "        \"Rumination Full Scale\"\n",
    "    ].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define searching experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_regressor(\n",
    "    X_train, y_train, X_test, y_test, regressor, regressor_params, base_steps, cv\n",
    "):\n",
    "\n",
    "    pipeline = Pipeline(steps=base_steps + [regressor])\n",
    "    param_grid = regressor_params\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        scoring={\"r2\", \"neg_mean_absolute_error\"},\n",
    "        refit=\"r2\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=10,\n",
    "        verbose=10,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    tested_regressors,\n",
    "    regressor_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    base_steps,\n",
    "    results_df,\n",
    "    cv=KFold(n_splits=3, shuffle=False)\n",
    "    #     function_name=\"-\",\n",
    "):\n",
    "\n",
    "    for (regressor, params) in tested_regressors:\n",
    "        print(f\"Rating {regressor} \\n\")\n",
    "        tested_params = {**regressor_params, **params}\n",
    "\n",
    "        grid_result = rate_regressor(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            regressor,\n",
    "            tested_params,\n",
    "            base_steps,\n",
    "            cv=cv,\n",
    "        )\n",
    "\n",
    "        #     predictions = grid_result.predict(X_test)\n",
    "        #     r2 = grid_result.score(X_test, y_test)\n",
    "        #     mae = mean_absolute_error(y_test, predictions)\n",
    "        #     r2_adj = r2_adjusted_scorer(y_test, predictions, len(X_test[0]), len(X_test))\n",
    "\n",
    "        best_estimator_index = grid_result.best_index_\n",
    "        mean_cv_r2 = grid_result.cv_results_[\"mean_test_r2\"][best_estimator_index]\n",
    "        std_cv_r2 = grid_result.cv_results_[\"std_test_r2\"][best_estimator_index]\n",
    "        mean_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        mean_train_r2 = grid_result.cv_results_[\"mean_train_r2\"][best_estimator_index]\n",
    "\n",
    "        print(f\"     Best parameters: {grid_result.best_params_}\")\n",
    "        print(f\"     mean r2: {mean_cv_r2}           ± {round(std_cv_r2,3)}\")\n",
    "        print(f\"     mean r2 train: {mean_train_r2}\")\n",
    "\n",
    "        cv_results = grid_result.cv_results_\n",
    "\n",
    "        # calculate p-value\n",
    "        scores_, pvalue_ = calculate_p_permutations(\n",
    "            grid_result.best_estimator_, X_train, y_train\n",
    "        )\n",
    "\n",
    "        #         show_validation_curves(grid_result.cv_results_, tested_params)\n",
    "\n",
    "        data = {\n",
    "            \"data_set\": dataset_name,\n",
    "            \"pipeline_name\": pipeline_name,\n",
    "            #             \"function\": function_name,\n",
    "            \"model\": regressor[0],\n",
    "            \"parameters\": grid_result.best_params_,\n",
    "            \"mean_cv_r2\": mean_cv_r2,\n",
    "            \"std_cv_r2\": std_cv_r2,\n",
    "            \"mean_cv_mae\": mean_cv_neg_mean_absolute_error,\n",
    "            \"std_cv_mae\": std_cv_neg_mean_absolute_error,\n",
    "            \"cv_results\": cv_results,\n",
    "            \"mean_train_r2\": mean_train_r2,\n",
    "            \"p-value\": pvalue_,\n",
    "            \"best_estimator\": grid_result.best_estimator_,\n",
    "        }\n",
    "\n",
    "        results_df = results_df.append(data, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data transformers - custom data transformation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "\n",
    "class LowpassFilter(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        fs = signal_frequency\n",
    "        cutoff = 45  # Hz\n",
    "        B, A = butter(\n",
    "            6, cutoff / (fs / 2), btype=\"low\", analog=False\n",
    "        )  # 6th order Butterworth low-pass\n",
    "\n",
    "        filtered_epochs_per_channel = []\n",
    "        for channel in X:\n",
    "            filtered_epochs = np.array(\n",
    "                [lfilter(B, A, epoch, axis=0) for epoch in channel]\n",
    "            )\n",
    "            filtered_epochs_per_channel.append(filtered_epochs)\n",
    "        filtered_epochs_per_channel = np.array(filtered_epochs_per_channel)\n",
    "        return filtered_epochs_per_channel\n",
    "\n",
    "\n",
    "class IcaPreprocessing(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "        return timepoints_per_channel.T\n",
    "\n",
    "\n",
    "class IcaPostprocessing(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, timepoints_count):\n",
    "        super().__init__()\n",
    "        self.timepoints_count = timepoints_count\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ica_transposed = X.T\n",
    "        ica_n_components = X.shape[1]\n",
    "\n",
    "        epochs_count = int(X_ica_transposed.shape[1] / self.timepoints_count)\n",
    "        data_per_channel = X_ica_transposed.reshape(\n",
    "            ica_n_components, epochs_count, self.timepoints_count\n",
    "        )\n",
    "\n",
    "        return data_per_channel\n",
    "\n",
    "\n",
    "class Cwt(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, mwt=\"morl\", cwt_density=2, cwt_octaves=6):\n",
    "        # for octaves=6, the highest frequency is 45.25 Hz\n",
    "        super().__init__()\n",
    "        self.mwt = mwt\n",
    "        self.cwt_density = cwt_density\n",
    "        self.cwt_octaves = cwt_octaves\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        cwt_per_channel = []\n",
    "        for data in X:\n",
    "            data_cwt = np.array(\n",
    "                [\n",
    "                    cwt(epoch, self.mwt, self.cwt_density, self.cwt_octaves)\n",
    "                    for epoch in data\n",
    "                ]\n",
    "            )\n",
    "            cwt_per_channel.append(data_cwt)\n",
    "        cwt_per_channel = np.array(cwt_per_channel)\n",
    "        return cwt_per_channel\n",
    "\n",
    "\n",
    "class CwtFeatureVectorizer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, feature_dict):\n",
    "        super().__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectorized_data = []\n",
    "        for data_cwt in X:\n",
    "            # cesium functions\n",
    "            feature_set_cwt = cesium.featurize.featurize_time_series(\n",
    "                times=None,\n",
    "                values=data_cwt,\n",
    "                errors=None,\n",
    "                features_to_use=list(self.feature_dict.keys()),\n",
    "                custom_functions=self.feature_dict,\n",
    "            )\n",
    "            features_per_epoch = feature_set_cwt.to_numpy()\n",
    "            vectorized_data.append(features_per_epoch)\n",
    "        vectorized_data = np.array(vectorized_data)\n",
    "        return vectorized_data\n",
    "\n",
    "\n",
    "# reshape data from (channels x epoch x features) to (epochs x channles x features)\n",
    "# and then flatten it to (epoch x channels*features)\n",
    "class PostprocessingTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectorized_data = np.stack(X, axis=1)\n",
    "        epochs_per_channel_feature = vectorized_data.reshape(\n",
    "            vectorized_data.shape[0], -1\n",
    "        )\n",
    "        return epochs_per_channel_feature\n",
    "\n",
    "\n",
    "class ChannelExtraction(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, channel_list):\n",
    "        super().__init__()\n",
    "        self.channel_list = channel_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        epochs_per_channels = np.transpose(X, (1, 0, 2))\n",
    "        epochs_per_selected_channels = []\n",
    "\n",
    "        for channel in self.channel_list:\n",
    "            this_data = epochs_per_channels[channel]\n",
    "            epochs_per_selected_channels.append(this_data)\n",
    "\n",
    "        epochs_per_selected_channels = np.array(epochs_per_selected_channels)\n",
    "        selected_channels_per_epoch = np.transpose(\n",
    "            epochs_per_selected_channels, (1, 0, 2)\n",
    "        )\n",
    "        #         print(f\"EXTRACTION {selected_channels_per_epoch.shape}\")\n",
    "        return selected_channels_per_epoch\n",
    "\n",
    "\n",
    "# swap channels and epochs axes: from epoch_channel_timepoints to channel_epoch_timepoints and vice versa\n",
    "class ChannelDataSwap(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data_channel_swaped = np.transpose(X, (1, 0, 2))\n",
    "        return data_channel_swaped\n",
    "\n",
    "\n",
    "class BinTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, step):\n",
    "        super().__init__()\n",
    "        self.step = step\n",
    "\n",
    "    def bin_epoch(self, epoch):\n",
    "        new_channels = []\n",
    "        for channel in epoch:\n",
    "            bins_channel = []\n",
    "            index = 0\n",
    "            while index + self.step < len(channel):\n",
    "                this_bin = np.mean(channel[index : index + self.step])\n",
    "                bins_channel.append(this_bin)\n",
    "                index += self.step\n",
    "            new_channels.append(bins_channel)\n",
    "        return new_channels\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        binned_data = np.array([self.bin_epoch(epoch) for epoch in X])\n",
    "        return binned_data\n",
    "\n",
    "\n",
    "# transforms energy of each sub-band into relative energy of sub-band\n",
    "def RelativeEnergyTransformer():\n",
    "    def transform(X):\n",
    "        vectorized_data = []\n",
    "\n",
    "        for epoch in X:\n",
    "            total_energy_of_epoch = np.sum(epoch)\n",
    "            sub_band_relative_energies = np.array(\n",
    "                [(sub_band_energy / total_energy_of_epoch) for sub_band_energy in epoch]\n",
    "            )\n",
    "            vectorized_data.append(sub_band_relative_energies)\n",
    "\n",
    "        vectorized_data = np.array(vectorized_data)\n",
    "        return vectorized_data\n",
    "\n",
    "    return FunctionTransformer(func=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "### Define architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA-bins + ICA-bins-cwt-features\n",
    "\n",
    "\n",
    "def ica_bins_features_steps(feature_function_dict):\n",
    "\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"ica_preprocessing\", IcaPreprocessing()),\n",
    "        #         (\"ica\", FastICA(random_state=random_state)),\n",
    "        (\"spatial_filter\", PCA(random_state=random_state)),\n",
    "        (\n",
    "            \"ica_postprocessing\",\n",
    "            IcaPostprocessing(timepoints_count=X_train.shape[-1]),\n",
    "        ),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\n",
    "            \"features\",\n",
    "            FeatureUnion(\n",
    "                [\n",
    "                    (\n",
    "                        \"bins\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\"channel_data_swap\", ChannelDataSwap()),\n",
    "                                (\"binning\", BinTransformer(step=step_tp)),\n",
    "                                (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                (\"postprocessing_bins\", PostprocessingTransformer()),\n",
    "                            ]\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"functions\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\"cwt\", Cwt()),\n",
    "                                (\n",
    "                                    \"cwt_feature\",\n",
    "                                    CwtFeatureVectorizer(\n",
    "                                        feature_dict=feature_function_dict\n",
    "                                    ),\n",
    "                                ),\n",
    "                                (\n",
    "                                    \"postprocessing_functions\",\n",
    "                                    PostprocessingTransformer(),\n",
    "                                ),\n",
    "                            ]\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERP-bins + ERP-bins-cwt-features\n",
    "\n",
    "\n",
    "def erp_bins_features_steps(feature_function_dict):\n",
    "\n",
    "    functions_base_steps = [\n",
    "        (\"cwt\", Cwt()),\n",
    "        (\n",
    "            \"cwt_feature\",\n",
    "            CwtFeatureVectorizer(feature_dict=feature_function_dict),\n",
    "        ),\n",
    "        (\"postprocessing_func\", PostprocessingTransformer()),\n",
    "    ]\n",
    "    functions_pipeline = Pipeline(steps=functions_base_steps)\n",
    "\n",
    "    bins_base_steps = [\n",
    "        (\"data_channel_swap_after_filter\", ChannelDataSwap()),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing_bins\", PostprocessingTransformer()),\n",
    "    ]\n",
    "    bins_pipeline = Pipeline(steps=bins_base_steps)\n",
    "\n",
    "    combined_features = FeatureUnion(\n",
    "        [(\"bins\", bins_pipeline), (\"functins\", functions_pipeline)]\n",
    "    )\n",
    "\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"data_channel_swap_filter\", ChannelDataSwap()),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"features\", combined_features),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial-filter-bins\n",
    "\n",
    "\n",
    "def spatial_filter_bins_steps(spatial_filter):\n",
    "\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"ica_preprocessing\", IcaPreprocessing()),\n",
    "        (\"spatial_filter\", spatial_filter),\n",
    "        (\n",
    "            \"ica_postprocessing\",\n",
    "            IcaPostprocessing(timepoints_count=X_train.shape[-1]),\n",
    "        ),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"channel_data_swap\", ChannelDataSwap()),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing\", PostprocessingTransformer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erp-bins\n",
    "def erp_bins_steps():\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"data_channel_swap_filter\", ChannelDataSwap()),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"data_channel_swap_after_filter\", ChannelDataSwap()),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing\", PostprocessingTransformer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate estimator HTML representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import estimator_html_repr\n",
    "\n",
    "# with open(\"my_estimator.html\", \"w\") as f:\n",
    "#     f.write(estimator_html_repr(Pipeline(this_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment setup with spatial filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial filter should be specified. Default: PCA\n",
    "# spatial_filter__n_components: maximum value should be specified manually\n",
    "\n",
    "this_spatial_filter = PCA(random_state=random_state)\n",
    "spatial_filter_max = 8\n",
    "\n",
    "\n",
    "regressor_params = dict(\n",
    "    spatial_filter__n_components=np.arange(7, spatial_filter_max, 1),\n",
    "    feature_selection__n_components=np.arange(1, 9, 1),\n",
    ")\n",
    "\n",
    "pipeline_name = \"PCA_\" + str(spatial_filter_max) + \"_bins\"\n",
    "\n",
    "# create steps of pipeline\n",
    "this_steps = spatial_filter_bins_steps(spatial_filter=this_spatial_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment setup without spatial filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_params = dict(\n",
    "    feature_selection__n_components=np.arange(3, 9, 1),\n",
    ")\n",
    "pipeline_name = \"ERP_bins\"\n",
    "\n",
    "# create steps of pipeline\n",
    "this_steps = erp_bins_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching best regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual search for parameters that limit overfitting the most - results write into *res* dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for alpha in [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4]:\n",
    "    for l1_ratio in [\n",
    "        0.0000001,\n",
    "        0.000001,\n",
    "        0.00001,\n",
    "        0.0001,\n",
    "        0.001,\n",
    "        0.01,\n",
    "        0.1,\n",
    "        0.3,\n",
    "        0.5,\n",
    "        0.7,\n",
    "        1,\n",
    "    ]:\n",
    "\n",
    "        en = (\"en\", ElasticNet(random_state=random_state))\n",
    "        en_params = dict(\n",
    "            en__alpha=[alpha],\n",
    "            en__l1_ratio=[l1_ratio],\n",
    "        )\n",
    "\n",
    "        tested_regressors = [(en, en_params)]\n",
    "\n",
    "        # rate different models\n",
    "        res = run_experiment(\n",
    "            tested_regressors,\n",
    "            regressor_params,\n",
    "            pipeline_name,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            this_steps,\n",
    "            res,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for alpha in [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4]:\n",
    "    for gamma in [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "\n",
    "        kr = (\"kr\", KernelRidge(kernel=\"rbf\"))\n",
    "        kr_params = dict(kr__alpha=[alpha], kr__gamma=[gamma])\n",
    "\n",
    "        tested_regressors = [\n",
    "            (kr, kr_params),\n",
    "        ]\n",
    "\n",
    "        # rate different models\n",
    "        res = run_experiment(\n",
    "            tested_regressors,\n",
    "            regressor_params,\n",
    "            pipeline_name,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            this_steps,\n",
    "            res,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for C in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]:\n",
    "    for epsilon in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "        #         for gamma in [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "\n",
    "        svr = (\"svr\", SVR())\n",
    "        svr_params = dict(\n",
    "            svr__kernel=[\"linear\"],\n",
    "            svr__C=[C],\n",
    "            svr__gamma=[\"scale\"],\n",
    "            svr__epsilon=[epsilon],\n",
    "        )\n",
    "\n",
    "        tested_regressors = [\n",
    "            (svr, svr_params),\n",
    "        ]\n",
    "\n",
    "        # rate different models\n",
    "        res = run_experiment(\n",
    "            tested_regressors,\n",
    "            regressor_params,\n",
    "            pipeline_name,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            this_steps,\n",
    "            res,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = (\"en\", ElasticNet(random_state=random_state))\n",
    "en_params = dict(\n",
    "    en__alpha=[0.00001],\n",
    "    en__l1_ratio=[0.0000001],\n",
    ")\n",
    "\n",
    "kr = (\"kr\", KernelRidge(kernel=\"rbf\"))\n",
    "kr_params = dict(kr__alpha=[0.4], kr__gamma=[0.00001])\n",
    "\n",
    "\n",
    "svr = (\"svr\", SVR())\n",
    "svr_params = dict(\n",
    "    svr__kernel=[\"linear\"],\n",
    "    svr__C=[0.0001],\n",
    "    svr__gamma=[\"scale\"],\n",
    "    svr__epsilon=[1],\n",
    ")\n",
    "\n",
    "tested_regressors = [\n",
    "    (svr, svr_params),\n",
    "    (kr, kr_params),\n",
    "    (en, en_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = run_experiment(\n",
    "    tested_regressors,\n",
    "    regressor_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    this_steps,\n",
    "    results_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle(\n",
    "    \"../../data/regression_PCA_15_all_4_bins_upto_8_features_\" + dataset_name + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_viz = results_df.drop(columns=[\"best_estimator\"])\n",
    "results_viz.to_pickle(\n",
    "    \"../../data/regression_PCA_15_all_4_bins_upto_8_features_visualization_\"\n",
    "    + dataset_name\n",
    "    + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations of pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_ICA = \"../../data/results_regression/regression_ICA_error.pkl\"\n",
    "results_ICA = pd.read_pickle(file_name_ICA)\n",
    "\n",
    "file_name_PCA = \"../../data/results_regression/regression_PCA_error.pkl\"\n",
    "results_PCA = pd.read_pickle(file_name_PCA)\n",
    "\n",
    "file_name_ERP = \"../../data/results_regression/regression_ERP_error.pkl\"\n",
    "results_ERP = pd.read_pickle(file_name_ERP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_ICA, results_PCA, results_ERP], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_func_df = results_df[\n",
    "    results_df[\"pipeline_name\"].isin(\n",
    "        [\"ERP_bins\", \"PCA_15_bins\", \"PCA_4_bins\", \"ICA_15_bins\", \"ICA_4_bins\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recalculate p-values with permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "permutation_results = []\n",
    "\n",
    "for index, row in results_without_func_df.iterrows():\n",
    "    estimator = row.best_estimator\n",
    "    current_r2 = row.mean_cv_r2\n",
    "    #     print(estimator)\n",
    "    cv_kf = KFold(n_splits=3, shuffle=False)\n",
    "\n",
    "    score_, perm_scores_, pvalue_ = permutation_test_score(\n",
    "        estimator,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=\"r2\",\n",
    "        cv=cv_kf,\n",
    "        n_permutations=1000,\n",
    "        n_jobs=11,\n",
    "    )\n",
    "    print(f\"Score: {score_}   df_score: {current_r2}  p_value: {pvalue_}  \")\n",
    "    permutation_results.append((score_, perm_scores_, pvalue_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_results = np.array(permutation_results)\n",
    "np.save(\"permutation_results.npy\", permutation_results, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, perm_scores, p_val = np.split(perm_res, indices_or_sections=3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_scores_df = pd.DataFrame(perm_scores)\n",
    "p_values_df = pd.DataFrame(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add info about permutation test results to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_func_df[\"p-value\"] = p_values_df\n",
    "results_without_func_df[\"permutation_score\"] = perm_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_func_df.to_pickle(\n",
    "    \"regression_all_results_without_functions_\" + dataset_name + \".pkl\"\n",
    ")\n",
    "results_without_func_df_viz = results_without_func_df.drop(columns=[\"best_estimator\"])\n",
    "results_without_func_df_viz.to_pickle(\n",
    "    \"regression_all_results_without_functions_viz\" + dataset_name + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of permutation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_func_df = pd.read_pickle(\n",
    "    \"regression_all_results_without_functions_\" + dataset_name + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best model in each pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (\n",
    "    results_without_func_df.groupby([\"pipeline_name\"])[\"mean_cv_r2\"].transform(max)\n",
    "    == results_without_func_df[\"mean_cv_r2\"]\n",
    ")\n",
    "best_model_in_pipeline_df = results_without_func_df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    5,\n",
    "    figsize=(52, 9),\n",
    "    facecolor=\"w\",\n",
    "    edgecolor=\"k\",\n",
    "    sharey=True,\n",
    "    sharex=True,\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.02, left=0.04)\n",
    "\n",
    "\n",
    "sns.set(font_scale=4)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "axs = axs.ravel()\n",
    "i = 0\n",
    "for index, row in best_model_in_pipeline_df.iterrows():\n",
    "    #     plt.figure(figsize=(15, 12))\n",
    "\n",
    "    sns.histplot(ax=axs[i], x=row.permutation_scores, bins=12, kde=True).set(\n",
    "        xlabel=\"$R^2$\"\n",
    "    )\n",
    "\n",
    "    axs[i].axvline(row[\"mean_cv_r2\"], -1, color=\"r\", linestyle=\"--\", linewidth=4)\n",
    "\n",
    "    hist = np.histogram(row.permutation_scores, bins=12)\n",
    "    hist_max = max(hist[0])\n",
    "\n",
    "    text = \"Score: \" + str(round(row[\"mean_cv_r2\"], 3))\n",
    "    axs[i].text(\n",
    "        0.9 * row[\"mean_cv_r2\"],\n",
    "        290,\n",
    "        s=text,\n",
    "        horizontalalignment=\"right\",\n",
    "        size=\"small\",\n",
    "        color=\"black\",\n",
    "        bbox=dict(boxstyle=\"round\", alpha=0.2),\n",
    "    )\n",
    "    pipeline_name = row.pipeline_name[:-5]\n",
    "    axs[i].set_title(\"Pipeline: \" + pipeline_name, pad=30)\n",
    "    i = i + 1\n",
    "\n",
    "fig.savefig(\"regression_permutation_scores.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuzalize rumination distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "sns.set(font_scale=3)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "sns.histplot(y_train, bins=15).set(xlabel=\"Rumination scores\")\n",
    "plt.savefig(\"rumination_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualization of features extracted by models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define index of model for visualization in results_without_func_df dataframe\n",
    "index = 9\n",
    "estimator = results_without_func_df.best_estimator[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pca_coeffs = estimator[\"feature_selection\"].components_\n",
    "pipeline_estimator_coeffs = estimator[\"en\"].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply coeffs from spatial filter and coeffs from estimator\n",
    "# to extract objective importance of each feature (where feature is bin value at given channel).\n",
    "\n",
    "multiplied_components = []\n",
    "for index in range(0, len(pipeline_estimator_coeffs)):\n",
    "    mul_component = (\n",
    "        pipeline_pca_coeffs[index]\n",
    "        * pipeline_estimator_coeffs[index]\n",
    "        #         * mean_features_sign\n",
    "    )\n",
    "    multiplied_components.append(mul_component)\n",
    "multiplied_components = np.array(multiplied_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape components to recover *channels x bins* structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplied_components = multiplied_components.reshape(\n",
    "    multiplied_components.shape[0], 3, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum data on components from feature extraction to get averaged time-features along the components.\n",
    "Since the components from feature extraction are weighted with the estimator coefficients, the data can be summed for the average results.\n",
    "\n",
    "Data will be *spatial_filter x bins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.negative(np.sum(multiplied_components, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of relationship between signal amplitudes and rumination.\n",
    "\n",
    "Blue color indicates that larger negative amplitude is associated with higher rumination level. Red color indicates that larger positive amplitude is associated with higher rumination level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(55, 7))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    left=None, bottom=None, right=None, top=None, wspace=0.05, hspace=None\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    ax=ax[0],\n",
    "    data=[data[0]],\n",
    "    center=0,\n",
    "    cmap=\"vlag\",\n",
    "    yticklabels=False,\n",
    "    cbar=False,\n",
    "    xticklabels=[\n",
    "        -100,\n",
    "        -50,\n",
    "        0,\n",
    "        50,\n",
    "        100,\n",
    "        150,\n",
    "        200,\n",
    "        250,\n",
    "        300,\n",
    "        350,\n",
    "        400,\n",
    "        450,\n",
    "        500,\n",
    "        550,\n",
    "        600,\n",
    "    ],\n",
    ")\n",
    "sns.heatmap(\n",
    "    ax=ax[1],\n",
    "    data=[data[1]],\n",
    "    center=0,\n",
    "    cmap=\"vlag\",\n",
    "    yticklabels=False,\n",
    "    cbar=False,\n",
    "    xticklabels=[\n",
    "        -100,\n",
    "        -50,\n",
    "        0,\n",
    "        50,\n",
    "        100,\n",
    "        150,\n",
    "        200,\n",
    "        250,\n",
    "        300,\n",
    "        350,\n",
    "        400,\n",
    "        450,\n",
    "        500,\n",
    "        550,\n",
    "        600,\n",
    "    ],\n",
    ")\n",
    "sns.heatmap(\n",
    "    ax=ax[2],\n",
    "    data=[data[2]],\n",
    "    center=0,\n",
    "    cmap=\"vlag\",\n",
    "    yticklabels=False,\n",
    "    cbar=False,\n",
    "    xticklabels=[\n",
    "        -100,\n",
    "        -50,\n",
    "        0,\n",
    "        50,\n",
    "        100,\n",
    "        150,\n",
    "        200,\n",
    "        250,\n",
    "        300,\n",
    "        350,\n",
    "        400,\n",
    "        450,\n",
    "        500,\n",
    "        550,\n",
    "        600,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"rumination_regression_features_coeffs_PCA_4_en.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of signal at spatial filter components / channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "\n",
    "pipeline_without_feature_extraction = estimator.steps[:-3]\n",
    "features = Pipeline(pipeline_without_feature_extraction).transform(X_train)\n",
    "\n",
    "spatial_filter_num_components = int(\n",
    "    features.shape[-1] / int(X_train.shape[-1] / step_tp)\n",
    ")\n",
    "mean_features = np.mean(features, axis=0)\n",
    "mean_features = mean_features.reshape(spatial_filter_num_components, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "components = [\"PCA 1\", \"PCA 2\", \"PCA 3\"]\n",
    "bucket_width_ms = 1 / 256 * 12 * 1000\n",
    "xs = np.array([(bucket_width_ms * x - 100) for x in range(len(mean_features[0]))])\n",
    "for index in range(len(mean_features)):\n",
    "    for subindex in range(len(mean_features[index])):\n",
    "        data.append(\n",
    "            {\n",
    "                \"x\": xs[subindex],\n",
    "                \"y\": np.negative(mean_features[index][subindex]),\n",
    "                \"component\": components[index],\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "g = sns.relplot(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    #     hue=\"Rumination\",\n",
    "    col=\"component\",\n",
    "    kind=\"line\",\n",
    "    data=df,\n",
    "    linewidth=7,\n",
    "    col_wrap=3,\n",
    "    zorder=0,\n",
    "    height=12,\n",
    "    aspect=1.5,\n",
    ")\n",
    "\n",
    "g.map(\n",
    "    plt.axhline, y=0, color=\".7\", dashes=(5, 5), zorder=0, linewidth=7\n",
    ").set_axis_labels(\"Time [ms]\", \"Amplitude [10*µV]\").set_titles(\"Component: {col_name}\")\n",
    "\n",
    "g.savefig(\"rumination_regression_components_signal_PCA_4_en.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
