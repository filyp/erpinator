{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# sns.set(rc={\"figure.figsize\": (20, 20)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(colors), 2):\n",
    "    r,g,b = colors[i]\n",
    "    pivot = max(r,g,b)\n",
    "    if r == pivot:\n",
    "        colors[i] = (r, min(g*1.2, 1), min(b*1.25, 1))\n",
    "    elif g == pivot:\n",
    "        colors[i] = (min(r*1.2, 1), g, min(b*1.4, 1))\n",
    "    else:\n",
    "        colors[i] = (min(r*1.25, 1), min(g*1.1, 1), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_transparent(x,y,col,data,ci='sd'):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    col_wrap = None if col==None else 5\n",
    "    \n",
    "    hue_categories = ['en not significant','en', 'kr not significant', 'kr', 'svr not significant', 'svr']\n",
    "    hue_categories_colors = colors\n",
    "\n",
    "    data = data.reset_index()\n",
    "    hue_column = \"statistical significance \"\n",
    "    data[hue_column] = data.apply(lambda row: row[x] if row[\"p-value\"] < 0.05 else row[x]+\" not significant\", axis=1)\n",
    "    \n",
    "    palette = dict(zip(hue_categories, hue_categories_colors))\n",
    "    ax = sns.catplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        col=col,\n",
    "        col_order=['ICA_15_bins', 'ICA_4_bins', 'PCA_15_bins', 'PCA_4_bins', 'ERP_bins'],\n",
    "        hue=hue_column,\n",
    "        ci=ci,\n",
    "        data=data, \n",
    "        kind='bar', \n",
    "        capsize=.05,\n",
    "        errwidth = 1,\n",
    "        legend=True,\n",
    "        col_wrap=col_wrap,\n",
    "        dodge=False,\n",
    "        palette=palette,\n",
    "        margin_titles=True\n",
    "    )\n",
    "    \n",
    "    ax.savefig(\"rumination_prediction_summary.png\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_ICA = \"../../data/results_regression/regression_ICA_visualization_error.pkl\"\n",
    "results_ICA = pd.read_pickle(file_name_ICA)\n",
    "\n",
    "file_name_PCA = \"../../data/results_regression/regression_PCA_visualization_error.pkl\"\n",
    "results_PCA = pd.read_pickle(file_name_PCA)\n",
    "\n",
    "file_name_ERP = \"../../data/results_regression/regression_ERP_visualization_error.pkl\"\n",
    "results_ERP = pd.read_pickle(file_name_ERP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_ICA, results_PCA, results_ERP], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"../../data/results_regression/regression_results_without_functions_visualization_error.pkl\"\n",
    "results_df = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting error bars - hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates=10000\n",
    "\n",
    "#duplicate observations to get good std bars\n",
    "dfCopy = results_df.loc[results_df.index.repeat(duplicates)].copy()\n",
    "dfCopy['mean_cv_r2'] = np.random.normal(dfCopy['mean_cv_r2'].values,dfCopy['std_cv_r2'].values)\n",
    "dfCopy['mean_cv_mae'] = np.random.normal(dfCopy['mean_cv_mae'].values,dfCopy['std_cv_mae'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines vs Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_transparent(x=\"model\", y=\"mean_cv_r2\", col=\"pipeline_name\", ci='sd', data=dfCopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_mae_list = []\n",
    "for index, row in results_df.iterrows():\n",
    "    cv_results_params = row.cv_results['params']\n",
    "    params = row.parameters\n",
    "        \n",
    "    index = cv_results_params.index(params)\n",
    "    mean_train_mae = row.cv_results['mean_train_neg_mean_absolute_error'][index]\n",
    "    mean_train_mae_list.append(abs(mean_train_mae))\n",
    "\n",
    "    print(mean_train_mae)\n",
    "mean_train_mae_list = np.array(mean_train_mae_list)\n",
    "mean_train_mae_df = pd.DataFrame(mean_train_mae_list)\n",
    "results_df['mean_train_mae'] = mean_train_mae_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_df = results_df[results_df['p-value'] <= 0.06]\n",
    "summarize_df = results_df\n",
    "summarize_df = summarize_df[[\"data_set\", \"mean_cv_r2\", \"mean_train_r2\", \"mean_cv_mae\", \"mean_train_mae\", \"pipeline_name\", \"model\", \"std_cv_r2\", \"std_cv_mae\", \"p-value\", 'parameters', \"external_test_r2\", \"external_test_correlation\", \"internal_correlation\"]]\n",
    "summarize_df = summarize_df.reset_index()\n",
    "columns_order = [\"data_set\", \"pipeline_name\", \"model\", \"mean_train_r2\", \"mean_cv_r2\", \"std_cv_r2\", \"mean_train_mae\", \"mean_cv_mae\", \"std_cv_mae\", \"internal_correlation\", \"p-value\", \"external_test_r2\", \"external_test_correlation\", 'parameters']\n",
    "summarize_df = summarize_df[columns_order].rename(columns = {'mean_train_r2': 'mean train R2', 'mean_cv_r2': 'mean test R2', \"std_cv_r2\": \"R2 std\", 'mean_train_mae': 'mean train MAE', 'mean_cv_mae': 'mean test MAE', \"std_cv_mae\": \"MAE std\", \"external_test_r2\" : \"External validation R2\", \"external_test_correlation\" : \"External validation r\", \"internal_correlation\" : 'r'}, inplace = False)\n",
    "\n",
    "summarize_df['# spatial filter components'] = summarize_df['parameters'].apply(lambda x: x['ica__n_components'] if x.get('ica__n_components') is not None else (x['spatial_filter__n_components'] if x.get('spatial_filter__n_components') is not None else '-' ))\n",
    "summarize_df['# selected features'] = summarize_df['parameters'].apply(lambda x: x['feature_selection__n_components'])\n",
    "summarize_df = summarize_df.drop(columns=['parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_df[\"p-value\"] = pd.to_numeric(summarize_df[\"p-value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_df = summarize_df.round({'mean train R2': 3, 'mean test R2': 3, 'R2 std': 3, 'mean train MAE': 3, 'mean test MAE': 3, 'MAE std': 3, 'p-value': 3,\n",
    "                                  \"External validation R2\" : 3, 'External validation r' : 3, 'r' : 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_df.to_csv(\n",
    "    \"regression_summary.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_scores = results_df['permutation_scores'].to_numpy()\n",
    "scores = results_df['mean_cv_r2'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = permutation_scores[14]\n",
    "test_score = scores[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={\"figure.figsize\": (40, 20)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.displot(test, bins=30, height=8.27, aspect=11.7/8.27, palette = sns.color_palette(\"Paired\"))\n",
    "ax.set(xlabel='Accuracy score', ylabel='Count')\n",
    "\n",
    "score_label = (f\"Score on original\\ndata: {test_score:.3f}\")\n",
    "\n",
    "plt.text(0.0001, 111.7, score_label, fontsize=12, bbox=dict(facecolor='blue', alpha=0.1, edgecolor='black', linewidth=2))\n",
    "plt.axvline(test_score, ls='--', color='r')\n",
    "\n",
    "# ax.savefig(\"probalility_distribution_ERP_en.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
