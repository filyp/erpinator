{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rumination classification - averaged participants' epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_order_list = [\n",
    "    \"Fp1\",\n",
    "    \"AF7\",\n",
    "    \"AF3\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    \"F5\",\n",
    "    \"F7\",\n",
    "    \"FT7\",\n",
    "    \"FC5\",\n",
    "    \"FC3\",\n",
    "    \"FC1\",\n",
    "    \"C1\",\n",
    "    \"C3\",\n",
    "    \"C5\",\n",
    "    \"T7\",\n",
    "    \"TP7\",\n",
    "    \"CP5\",\n",
    "    \"CP3\",\n",
    "    \"CP1\",\n",
    "    \"P1\",\n",
    "    \"P3\",\n",
    "    \"P5\",\n",
    "    \"P7\",\n",
    "    \"P9\",\n",
    "    \"PO7\",\n",
    "    \"PO3\",\n",
    "    \"O1\",\n",
    "    \"Iz\",\n",
    "    \"Oz\",\n",
    "    \"POz\",\n",
    "    \"Pz\",\n",
    "    \"CPz\",\n",
    "    \"Fpz\",\n",
    "    \"Fp2\",\n",
    "    \"AF8\",\n",
    "    \"AF4\",\n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"F4\",\n",
    "    \"F6\",\n",
    "    \"F8\",\n",
    "    \"FT8\",\n",
    "    \"FC6\",\n",
    "    \"FC4\",\n",
    "    \"FC2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"C4\",\n",
    "    \"C6\",\n",
    "    \"T8\",\n",
    "    \"TP8\",\n",
    "    \"CP6\",\n",
    "    \"CP4\",\n",
    "    \"CP2\",\n",
    "    \"P2\",\n",
    "    \"P4\",\n",
    "    \"P6\",\n",
    "    \"P8\",\n",
    "    \"P10\",\n",
    "    \"PO8\",\n",
    "    \"PO4\",\n",
    "    \"O2\",\n",
    "]\n",
    "channels_dict = dict(zip(channels_order_list, np.arange(1, 64, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define significant channels - rest will be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_box = [\n",
    "    \"F1\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"FC1\",\n",
    "    \"FCz\",\n",
    "    \"FC2\",\n",
    "    \"C1\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"CP1\",\n",
    "    \"CPz\",\n",
    "    \"CP2\",\n",
    "    \"P1\",\n",
    "    \"Pz\",\n",
    "    \"P2\",\n",
    "]\n",
    "significant_channels = [channels_dict[channel] for channel in red_box]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df_mean\"\n",
    "pickled_data_filename = \"../../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average participants' error and correct epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_epochs_df = (\n",
    "    epochs_df.groupby(\n",
    "        [\"id\", \"marker\"],\n",
    "        sort=False,\n",
    "    )\n",
    "    .apply(\n",
    "        lambda group_df: pd.Series(\n",
    "            {\n",
    "                \"epoch\": np.mean(group_df[\"epoch\"]),\n",
    "                \"Rumination Full Scale\": np.mean(group_df[\"Rumination Full Scale\"]),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "## Training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard features for EEG analysis provided by Guo et al. (2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_signal(t, m, e):\n",
    "    return np.std(m)\n",
    "\n",
    "\n",
    "def abs_diffs_signal(t, m, e):\n",
    "    return np.sum(np.abs(np.diff(m)))\n",
    "\n",
    "\n",
    "def mean_energy_signal(t, m, e):\n",
    "    return np.mean(m ** 2)\n",
    "\n",
    "\n",
    "def skew_signal(t, m, e):\n",
    "    return scipy.stats.skew(m)\n",
    "\n",
    "\n",
    "def mean_signal(t, m, e):\n",
    "    return np.mean(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guo_features = {\n",
    "    \"mean\": mean_signal,\n",
    "    \"std\": std_signal,\n",
    "    \"mean_energy\": mean_energy_signal,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_in_ms = 50\n",
    "step_tp = int(signal_frequency * step_in_ms / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate p-value with permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "\n",
    "def calculate_p_permutations(estimator, X, y, cv=3, n_permutations=100, n_jobs=10):\n",
    "\n",
    "    score_, perm_scores_, pvalue_ = permutation_test_score(\n",
    "        estimator, X, y, cv=cv, n_permutations=n_permutations, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # summarize\n",
    "    print(f\"     The permutation P-value is = {pvalue_:.3f}\")\n",
    "\n",
    "    return score_, pvalue_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation curves - for parameters' insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pooled_var(stds):\n",
    "    # https://en.wikipedia.org/wiki/Pooled_variance#Pooled_standard_deviation\n",
    "    n = 5  # size of each group\n",
    "    return np.sqrt(sum((n - 1) * (stds ** 2)) / len(stds) * (n - 1))\n",
    "\n",
    "\n",
    "def show_validation_curves(cv_results, grid_params):\n",
    "\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    results = [\n",
    "        \"mean_test_balanced_accuracy\",\n",
    "        \"mean_train_balanced_accuracy\",\n",
    "        \"std_test_balanced_accuracy\",\n",
    "        \"std_train_balanced_accuracy\",\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1, len(grid_params), figsize=(5 * len(grid_params), 7), sharey=\"row\"\n",
    "    )\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=25)\n",
    "\n",
    "    for idx, (param_name, param_range) in enumerate(grid_params.items()):\n",
    "        grouped_df = df.groupby(f\"param_{param_name}\")[results].agg(\n",
    "            {\n",
    "                \"mean_train_balanced_accuracy\": \"mean\",\n",
    "                \"mean_test_balanced_accuracy\": \"mean\",\n",
    "                \"std_train_balanced_accuracy\": pooled_var,\n",
    "                \"std_test_balanced_accuracy\": pooled_var,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        previous_group = df.groupby(f\"param_{param_name}\")[results]\n",
    "        axes[idx].set_xlabel(param_name, fontsize=10)\n",
    "        axes[idx].set_ylim(0.0, 1.1)\n",
    "        axes[idx].set_xscale(\"log\")\n",
    "        lw = 2\n",
    "        axes[idx].plot(\n",
    "            param_range,\n",
    "            grouped_df[\"mean_train_balanced_accuracy\"],\n",
    "            label=\"Training score\",\n",
    "            color=\"darkorange\",\n",
    "            lw=lw,\n",
    "        )\n",
    "        axes[idx].fill_between(\n",
    "            param_range,\n",
    "            grouped_df[\"mean_train_balanced_accuracy\"]\n",
    "            - grouped_df[\"std_train_balanced_accuracy\"],\n",
    "            grouped_df[\"mean_train_balanced_accuracy\"]\n",
    "            + grouped_df[\"std_train_balanced_accuracy\"],\n",
    "            alpha=0.2,\n",
    "            color=\"darkorange\",\n",
    "            lw=lw,\n",
    "        )\n",
    "        axes[idx].plot(\n",
    "            param_range,\n",
    "            grouped_df[\"mean_test_balanced_accuracy\"],\n",
    "            label=\"Cross-validation score\",\n",
    "            color=\"navy\",\n",
    "            lw=lw,\n",
    "        )\n",
    "        axes[idx].fill_between(\n",
    "            param_range,\n",
    "            grouped_df[\"mean_test_balanced_accuracy\"]\n",
    "            - grouped_df[\"std_test_balanced_accuracy\"],\n",
    "            grouped_df[\"mean_test_balanced_accuracy\"]\n",
    "            + grouped_df[\"std_test_balanced_accuracy\"],\n",
    "            alpha=0.2,\n",
    "            color=\"navy\",\n",
    "            lw=lw,\n",
    "        )\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle(\"Validation curves\", fontsize=40)\n",
    "    fig.legend(handles, labels, loc=8, ncol=2, fontsize=20)\n",
    "\n",
    "    fig.subplots_adjust(bottom=0.25, top=0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERROR\n",
    "dataset_name = \"correct\" if dataset == CORRECT else \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(\n",
    "    averaged_epochs_df[averaged_epochs_df[\"marker\"] == dataset][\"epoch\"].to_list()\n",
    ")\n",
    "y_train = np.array(\n",
    "    averaged_epochs_df[averaged_epochs_df[\"marker\"] == dataset][\n",
    "        \"Rumination Full Scale\"\n",
    "    ].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data by median into two groups: high/low rumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumination_median = np.median(y_train)\n",
    "HIGH = 1\n",
    "LOW = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train)):\n",
    "    if y_train[i] < rumination_median:\n",
    "        y_train[i] = LOW\n",
    "    else:\n",
    "        y_train[i] = HIGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "class LowpassFilter(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        fs = signal_frequency\n",
    "        cutoff = 45  # Hz\n",
    "        B, A = butter(\n",
    "            6, cutoff / (fs / 2), btype=\"low\", analog=False\n",
    "        )  # 6th order Butterworth low-pass\n",
    "\n",
    "        filtered_epochs_per_channel = []\n",
    "        for channel in X:\n",
    "            filtered_epochs = np.array(\n",
    "                [lfilter(B, A, epoch, axis=0) for epoch in channel]\n",
    "            )\n",
    "            filtered_epochs_per_channel.append(filtered_epochs)\n",
    "        filtered_epochs_per_channel = np.array(filtered_epochs_per_channel)\n",
    "        return filtered_epochs_per_channel\n",
    "\n",
    "\n",
    "class IcaPreprocessing(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "        return timepoints_per_channel.T\n",
    "\n",
    "\n",
    "class IcaPostprocessing(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, timepoints_count):\n",
    "        super().__init__()\n",
    "        self.timepoints_count = timepoints_count\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ica_transposed = X.T\n",
    "        ica_n_components = X.shape[1]\n",
    "\n",
    "        epochs_count = int(X_ica_transposed.shape[1] / self.timepoints_count)\n",
    "        data_per_channel = X_ica_transposed.reshape(\n",
    "            ica_n_components, epochs_count, self.timepoints_count\n",
    "        )\n",
    "\n",
    "        return data_per_channel\n",
    "\n",
    "\n",
    "class Cwt(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, mwt=\"morl\", cwt_density=2, cwt_octaves=6):\n",
    "        # for octaves=6, the highest frequency is 45.25 Hz\n",
    "        super().__init__()\n",
    "        self.mwt = mwt\n",
    "        self.cwt_density = cwt_density\n",
    "        self.cwt_octaves = cwt_octaves\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        cwt_per_channel = []\n",
    "        for data in X:\n",
    "            data_cwt = np.array(\n",
    "                [\n",
    "                    cwt(epoch, self.mwt, self.cwt_density, self.cwt_octaves)\n",
    "                    for epoch in data\n",
    "                ]\n",
    "            )\n",
    "            cwt_per_channel.append(data_cwt)\n",
    "        cwt_per_channel = np.array(cwt_per_channel)\n",
    "        return cwt_per_channel\n",
    "\n",
    "\n",
    "class CwtFeatureVectorizer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, feature_dict):\n",
    "        super().__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectorized_data = []\n",
    "        for data_cwt in X:\n",
    "            # cesium functions\n",
    "            feature_set_cwt = cesium.featurize.featurize_time_series(\n",
    "                times=None,\n",
    "                values=data_cwt,\n",
    "                errors=None,\n",
    "                features_to_use=list(self.feature_dict.keys()),\n",
    "                custom_functions=self.feature_dict,\n",
    "            )\n",
    "            features_per_epoch = feature_set_cwt.to_numpy()\n",
    "            vectorized_data.append(features_per_epoch)\n",
    "        vectorized_data = np.array(vectorized_data)\n",
    "        return vectorized_data\n",
    "\n",
    "\n",
    "# reshape data from (channels x epoch x features) to (epochs x channles x features)\n",
    "# and then flatten it to (epoch x channels*features)\n",
    "class PostprocessingTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectorized_data = np.stack(X, axis=1)\n",
    "        epochs_per_channel_feature = vectorized_data.reshape(\n",
    "            vectorized_data.shape[0], -1\n",
    "        )\n",
    "        return epochs_per_channel_feature\n",
    "\n",
    "\n",
    "class ChannelExtraction(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, channel_list):\n",
    "        super().__init__()\n",
    "        self.channel_list = channel_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        epochs_per_channels = np.transpose(X, (1, 0, 2))\n",
    "        epochs_per_selected_channels = []\n",
    "\n",
    "        for channel in self.channel_list:\n",
    "            this_data = epochs_per_channels[channel]\n",
    "            epochs_per_selected_channels.append(this_data)\n",
    "\n",
    "        epochs_per_selected_channels = np.array(epochs_per_selected_channels)\n",
    "        selected_channels_per_epoch = np.transpose(\n",
    "            epochs_per_selected_channels, (1, 0, 2)\n",
    "        )\n",
    "        #         print(f\"EXTRACTION {selected_channels_per_epoch.shape}\")\n",
    "        return selected_channels_per_epoch\n",
    "\n",
    "\n",
    "# swap channels and epochs axes: from epoch_channel_timepoints to channel_epoch_timepoints and vice versa\n",
    "class ChannelDataSwap(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data_channel_swaped = np.transpose(X, (1, 0, 2))\n",
    "        return data_channel_swaped\n",
    "\n",
    "\n",
    "class BinTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, step):\n",
    "        super().__init__()\n",
    "        self.step = step\n",
    "\n",
    "    def bin_epoch(self, epoch):\n",
    "        new_channels = []\n",
    "        for channel in epoch:\n",
    "            bins_channel = []\n",
    "            index = 0\n",
    "            while index + self.step < len(channel):\n",
    "                this_bin = np.mean(channel[index : index + self.step])\n",
    "                bins_channel.append(this_bin)\n",
    "                index += self.step\n",
    "            new_channels.append(bins_channel)\n",
    "        return new_channels\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        binned_data = np.array([self.bin_epoch(epoch) for epoch in X])\n",
    "        return binned_data\n",
    "\n",
    "\n",
    "# transforms energy of each sub-band into relative energy of sub-band\n",
    "def RelativeEnergyTransformer():\n",
    "    def transform(X):\n",
    "        vectorized_data = []\n",
    "\n",
    "        for epoch in X:\n",
    "            total_energy_of_epoch = np.sum(epoch)\n",
    "            sub_band_relative_energies = np.array(\n",
    "                [(sub_band_energy / total_energy_of_epoch) for sub_band_energy in epoch]\n",
    "            )\n",
    "            vectorized_data.append(sub_band_relative_energies)\n",
    "\n",
    "        vectorized_data = np.array(vectorized_data)\n",
    "        return vectorized_data\n",
    "\n",
    "    return FunctionTransformer(func=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "### Define searching experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_classifier(\n",
    "    X_train, y_train, X_test, y_test, classifier, classifier_params, base_steps, cv=5\n",
    "):\n",
    "    # define cross-validation method\n",
    "    cv_skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "    pipeline = Pipeline(steps=base_steps + [classifier])\n",
    "    param_grid = classifier_params\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv_skf,\n",
    "        scoring={\"balanced_accuracy\", \"precision\"},\n",
    "        refit=\"balanced_accuracy\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=10,\n",
    "        verbose=10,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    tested_classifiers,\n",
    "    classifier_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    base_steps,\n",
    "    results_df,\n",
    "    function_name=\"-\",\n",
    "):\n",
    "\n",
    "    for (classifier, params) in tested_classifiers:\n",
    "        print(f\"Rating {classifier} \\n\")\n",
    "        tested_params = {**classifier_params, **params}\n",
    "\n",
    "        grid_result = rate_classifier(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            classifier,\n",
    "            tested_params,\n",
    "            base_steps,\n",
    "            cv=2,\n",
    "        )\n",
    "\n",
    "        # pull out the most important metrics\n",
    "        best_estimator_index = grid_result.best_index_\n",
    "        mean_cv_balanced_accuracy = grid_result.cv_results_[\n",
    "            \"mean_test_balanced_accuracy\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_balanced_accuracy = grid_result.cv_results_[\n",
    "            \"std_test_balanced_accuracy\"\n",
    "        ][best_estimator_index]\n",
    "        mean_cv_precision = grid_result.cv_results_[\"mean_test_precision\"][\n",
    "            best_estimator_index\n",
    "        ]\n",
    "        std_cv_precision = grid_result.cv_results_[\"std_test_precision\"][\n",
    "            best_estimator_index\n",
    "        ]\n",
    "        mean_train_balanced_accuracy = grid_result.cv_results_[\n",
    "            \"mean_train_balanced_accuracy\"\n",
    "        ][best_estimator_index]\n",
    "\n",
    "        # print results\n",
    "        print(f\"     Best parameters: {grid_result.best_params_}\")\n",
    "        print(\n",
    "            f\"     mean acc: {mean_cv_balanced_accuracy}           ± {round(std_cv_balanced_accuracy,3)}\"\n",
    "        )\n",
    "        print(f\"     mean acc train: {mean_train_balanced_accuracy}\")\n",
    "\n",
    "        cv_results = grid_result.cv_results_\n",
    "\n",
    "        # calculate p-value\n",
    "        scores_, pvalue_ = calculate_p_permutations(\n",
    "            grid_result.best_estimator_, X_train, y_train\n",
    "        )\n",
    "\n",
    "        #         show_validation_curves(grid_result.cv_results_, tested_params)\n",
    "\n",
    "        # save results into dataframe\n",
    "        data = {\n",
    "            \"data_set\": dataset_name,\n",
    "            \"pipeline_name\": pipeline_name,\n",
    "            \"model\": classifier[0],\n",
    "            \"parameters\": grid_result.best_params_,\n",
    "            \"mean_cv_balanced_accuracy\": mean_cv_balanced_accuracy,\n",
    "            \"std_cv_balanced_accuracy\": std_cv_balanced_accuracy,\n",
    "            \"mean_cv_precision\": mean_cv_precision,\n",
    "            \"std_cv_precision\": std_cv_precision,\n",
    "            \"cv_results\": cv_results,\n",
    "            \"mean_train_balanced_accuracy\": mean_train_balanced_accuracy,\n",
    "            \"p-value\": pvalue_,\n",
    "            \"best_estimator\": grid_result.best_estimator_,\n",
    "            #             \"t-stats\": t_statistics,\n",
    "        }\n",
    "\n",
    "        results_df = results_df.append(data, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "### Define architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERP-bins + ERP-bins-cwt-features\n",
    "\n",
    "\n",
    "def erp_bins_features_steps(feature_function_dict):\n",
    "\n",
    "    functions_base_steps = [\n",
    "        (\"cwt\", Cwt()),\n",
    "        (\n",
    "            \"cwt_feature\",\n",
    "            CwtFeatureVectorizer(feature_dict=feature_function_dict),\n",
    "        ),\n",
    "        (\"postprocessing_func\", PostprocessingTransformer()),\n",
    "    ]\n",
    "    functions_pipeline = Pipeline(steps=functions_base_steps)\n",
    "\n",
    "    bins_base_steps = [\n",
    "        (\"data_channel_swap_after_filter\", ChannelDataSwap()),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing_bins\", PostprocessingTransformer()),\n",
    "    ]\n",
    "    bins_pipeline = Pipeline(steps=bins_base_steps)\n",
    "\n",
    "    combined_features = FeatureUnion(\n",
    "        [(\"bins\", bins_pipeline), (\"functins\", functions_pipeline)]\n",
    "    )\n",
    "\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"data_channel_swap_filter\", ChannelDataSwap()),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"features\", combined_features),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erp-bins\n",
    "def erp_bins_steps():\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"data_channel_swap_filter\", ChannelDataSwap()),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"data_channel_swap_after_filter\", ChannelDataSwap()),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing\", PostprocessingTransformer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA-bins + ICA-bins-cwt-features\n",
    "\n",
    "\n",
    "def ica_bins_features_steps(feature_function_dict):\n",
    "\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"ica_preprocessing\", IcaPreprocessing()),\n",
    "        #         (\"ica\", FastICA(random_state=random_state)),\n",
    "        (\"spatial_filter\", PCA(random_state=random_state)),\n",
    "        (\n",
    "            \"ica_postprocessing\",\n",
    "            IcaPostprocessing(timepoints_count=X_train.shape[-1]),\n",
    "        ),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\n",
    "            \"features\",\n",
    "            FeatureUnion(\n",
    "                [\n",
    "                    (\n",
    "                        \"bins\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\"channel_data_swap\", ChannelDataSwap()),\n",
    "                                (\"binning\", BinTransformer(step=step_tp)),\n",
    "                                (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                (\"postprocessing_bins\", PostprocessingTransformer()),\n",
    "                            ]\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"functions\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\"cwt\", Cwt()),\n",
    "                                (\n",
    "                                    \"cwt_feature\",\n",
    "                                    CwtFeatureVectorizer(\n",
    "                                        feature_dict=feature_function_dict\n",
    "                                    ),\n",
    "                                ),\n",
    "                                (\n",
    "                                    \"postprocessing_functions\",\n",
    "                                    PostprocessingTransformer(),\n",
    "                                ),\n",
    "                            ]\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial-filter-bins\n",
    "\n",
    "\n",
    "def spatial_filter_bins_steps(spatial_filter):\n",
    "\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"ica_preprocessing\", IcaPreprocessing()),\n",
    "        (\"spatial_filter\", spatial_filter),\n",
    "        (\n",
    "            \"ica_postprocessing\",\n",
    "            IcaPostprocessing(timepoints_count=X_train.shape[-1]),\n",
    "        ),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"channel_data_swap\", ChannelDataSwap()),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing\", PostprocessingTransformer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment setup with spatial filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial filter should be specified. Default: PCA\n",
    "# spatial_filter__n_components: maximum value should be specified manually\n",
    "\n",
    "this_spatial_filter = PCA(random_state=random_state)\n",
    "spatial_filter_max = 15\n",
    "\n",
    "classifier_params = dict(\n",
    "    spatial_filter__n_components=np.arange(1, spatial_filter_max, 2),\n",
    "    feature_selection__n_components=np.arange(3, 9, 1),\n",
    ")\n",
    "\n",
    "pipeline_name = \"PCA_\" + str(spatial_filter_max) + \"_bins\"\n",
    "\n",
    "# create steps of pipeline\n",
    "this_steps = spatial_filter_bins_steps(spatial_filter=this_spatial_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment setup without spatial filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_params = dict(\n",
    "    feature_selection__n_components=np.arange(3, 9, 1),\n",
    ")\n",
    "pipeline_name = \"ERP_bins\"\n",
    "\n",
    "# create steps of pipeline\n",
    "this_steps = erp_bins_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching best regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual search for parameters that limit overfitting the most - results write into *res* dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for C in [0.0001, 0.001, 0.01, 0.1, 1, 10]:\n",
    "    for gamma in [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "\n",
    "        svc = (\"svc\", SVC())\n",
    "        svc_params = dict(\n",
    "            svc__kernel=[\"rbf\"],\n",
    "            svc__C=[C],\n",
    "            svc__gamma=[gamma],\n",
    "        )\n",
    "\n",
    "        tested_classifiers = [\n",
    "            (svc, svc_params),\n",
    "        ]\n",
    "\n",
    "        # rate different models\n",
    "        res = run_experiment(\n",
    "            tested_classifiers,\n",
    "            classifier_params,\n",
    "            pipeline_name,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            this_steps,\n",
    "            res,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for C in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "\n",
    "    svc = (\"svc\", SVC())\n",
    "    svc_params = dict(\n",
    "        svc__kernel=[\"linear\"],\n",
    "        svc__C=[C],\n",
    "        svc__gamma=[\"scale\"],\n",
    "    )\n",
    "\n",
    "    tested_classifiers = [\n",
    "        (svc, svc_params),\n",
    "    ]\n",
    "\n",
    "    # rate different models\n",
    "    res = run_experiment(\n",
    "        tested_classifiers,\n",
    "        classifier_params,\n",
    "        pipeline_name,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        dataset_name,\n",
    "        this_steps,\n",
    "        res,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for C in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for l1_ratio in [\n",
    "        0.0000001,\n",
    "        0.000001,\n",
    "        0.00001,\n",
    "        0.0001,\n",
    "        0.001,\n",
    "        0.01,\n",
    "        0.1,\n",
    "        0.3,\n",
    "        0.5,\n",
    "        0.7,\n",
    "        1,\n",
    "    ]:\n",
    "\n",
    "        lr = (\"lr\", LogisticRegression())\n",
    "        lr_params = dict(\n",
    "            lr__penalty=[\"elasticnet\"],\n",
    "            lr__solver=[\"saga\"],\n",
    "            lr__l1_ratio=[l1_ratio],\n",
    "            lr__C=[C],\n",
    "            lr__random_state=[random_state],\n",
    "        )\n",
    "\n",
    "        tested_classifiers = [(lr, lr_params)]\n",
    "\n",
    "        # rate different models\n",
    "        res = run_experiment(\n",
    "            tested_classifiers,\n",
    "            classifier_params,\n",
    "            pipeline_name,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            this_steps,\n",
    "            res,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_rbf = (\"svc_rbf\", SVC())\n",
    "svc_rbf_params = dict(\n",
    "    svc_rbf__kernel=[\"rbf\"],\n",
    "    svc_rbf__C=[1],\n",
    "    svc_rbf__gamma=[0.0000001],\n",
    ")\n",
    "\n",
    "svc_lin = (\"svc_lin\", SVC())\n",
    "svc_lin_params = dict(\n",
    "    svc_lin__kernel=[\"linear\"],\n",
    "    svc_lin__C=[0.01],\n",
    "    svc_lin__gamma=[\"scale\"],\n",
    ")\n",
    "\n",
    "\n",
    "lr_en = (\"lr_en\", LogisticRegression())\n",
    "lr_en_params = dict(\n",
    "    lr_en__penalty=[\"elasticnet\"],\n",
    "    lr_en__solver=[\"saga\"],\n",
    "    lr_en__C=[0.01],\n",
    "    lr_en__l1_ratio=[0.000001],\n",
    "    lr_en__random_state=[random_state],\n",
    ")\n",
    "\n",
    "\n",
    "lr = (\"lr\", LogisticRegression())\n",
    "lr_params = dict(lr__C=[0.01])\n",
    "\n",
    "\n",
    "tested_classifiers = [\n",
    "    (svc_rbf, svc_rbf_params),\n",
    "    (svc_lin, svc_lin_params),\n",
    "    (lr_en, lr_en_params),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate different models\n",
    "results_df = run_experiment(\n",
    "    tested_classifiers,\n",
    "    classifier_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    this_steps,\n",
    "    results_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle(\"../../data/classification_PCA_\" + dataset_name + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_viz = results_df.drop(columns=[\"best_estimator\"])\n",
    "results_viz.to_pickle(\n",
    "    \"../../data/classification_PCA_vizualization_\" + dataset_name + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations of pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_ICA = \"../../data/results_classification/classification_ICA_error.pkl\"\n",
    "results_ICA = pd.read_pickle(file_name_ICA)\n",
    "\n",
    "file_name_PCA = \"../../data/results_classification/classification_PCA_error.pkl\"\n",
    "results_PCA = pd.read_pickle(file_name_PCA)\n",
    "\n",
    "file_name_ERP = \"../../data/results_classification/classification_ERP_error.pkl\"\n",
    "results_ERP = pd.read_pickle(file_name_ERP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_ICA, results_PCA, results_ERP], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_func_df = results_df[\n",
    "    results_df[\"pipeline_name\"].isin(\n",
    "        [\"ERP_bins\", \"PCA_15_bins\", \"PCA_4_bins\", \"ICA_15_bins\", \"ICA_4_bins\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recalculate p-values with permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_results = []\n",
    "\n",
    "for index, row in results_without_func_df.iterrows():\n",
    "    estimator = row.best_estimator\n",
    "    current_acc = row.mean_cv_balanced_accuracy\n",
    "    #     print(estimator)\n",
    "    cv_skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "    score_, perm_scores_, pvalue_ = permutation_test_score(\n",
    "        estimator,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=cv_skf,\n",
    "        n_permutations=1000,\n",
    "        n_jobs=11,\n",
    "    )\n",
    "    print(f\"Score: {score_}   df_score: {current_acc}  p_value: {pvalue_}  \")\n",
    "    permutation_results.append((score_, perm_scores_, pvalue_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_results = np.array(permutation_results)\n",
    "np.save(\"permutation_results.npy\", permutation_results, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, perm_scores, p_val = np.split(permutation_results, indices_or_sections=3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_scores_df = pd.DataFrame(perm_scores)\n",
    "p_values_df = pd.DataFrame(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add info about permutation results to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_func_df[\"p-value\"] = p_values_df\n",
    "results_without_func_df[\"permutation_score\"] = perm_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe into pickle in two versions: with and without pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_func_df.to_pickle(\n",
    "    \"classification_all_results_without_functions_\" + dataset_name + \".pkl\"\n",
    ")\n",
    "results_without_func_df_viz = results_without_func_df.drop(columns=[\"best_estimator\"])\n",
    "results_without_func_df_viz.to_pickle(\n",
    "    \"classification_all_results_without_functions_viz\" + dataset_name + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of permutation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_func_df = pd.read_pickle(\n",
    "    \"classification_all_results_without_functions_\" + dataset_name + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best model in each pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (\n",
    "    results_without_func_df.groupby([\"pipeline_name\"])[\n",
    "        \"mean_cv_balanced_accuracy\"\n",
    "    ].transform(max)\n",
    "    == results_without_func_df[\"mean_cv_balanced_accuracy\"]\n",
    ")\n",
    "best_model_in_pipeline_df = results_without_func_df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    5,\n",
    "    figsize=(52, 9),\n",
    "    facecolor=\"w\",\n",
    "    edgecolor=\"k\",\n",
    "    sharey=True,\n",
    "    sharex=True,\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.02, left=0.05)\n",
    "\n",
    "\n",
    "sns.set(font_scale=4)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "axs = axs.ravel()\n",
    "i = 0\n",
    "for index, row in best_model_in_pipeline_df.iterrows():\n",
    "    #     plt.figure(figsize=(15, 12))\n",
    "\n",
    "    sns.histplot(ax=axs[i], x=row.permutation_score, bins=12, kde=True).set(\n",
    "        xlabel=\"Accuracy\"\n",
    "    )\n",
    "\n",
    "    axs[i].axvline(\n",
    "        row[\"mean_cv_balanced_accuracy\"], -1, color=\"r\", linestyle=\"--\", linewidth=4\n",
    "    )\n",
    "\n",
    "    hist = np.histogram(row.permutation_score, bins=12)\n",
    "    hist_max = max(hist[0])\n",
    "\n",
    "    text = \"Score: \" + str(round(row[\"mean_cv_balanced_accuracy\"], 3))\n",
    "    axs[i].text(\n",
    "        0.97 * row[\"mean_cv_balanced_accuracy\"],\n",
    "        220,\n",
    "        s=text,\n",
    "        horizontalalignment=\"right\",\n",
    "        size=\"small\",\n",
    "        color=\"black\",\n",
    "        bbox=dict(boxstyle=\"round\", alpha=0.2),\n",
    "    )\n",
    "    pipeline_name = row.pipeline_name[:-5]\n",
    "    axs[i].set_title(\"Pipeline: \" + pipeline_name, pad=30)\n",
    "    i = i + 1\n",
    "\n",
    "fig.savefig(\"classification_permutation_scores.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualization of features extracted by models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define index of model for visualization in results_without_func_df dataframe\n",
    "index = 13\n",
    "estimator = results_without_func_df.best_estimator[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pca_coeffs = estimator[\"feature_selection\"].components_\n",
    "pipeline_estimator_coeffs = estimator[\"svc_lin\"].coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply coeffs from spatial filter and coeffs from estimator\n",
    "# to extract objective importance of each feature (where feature is bin value at given channel).\n",
    "\n",
    "multiplied_components = []\n",
    "for index in range(0, len(pipeline_estimator_coeffs)):\n",
    "    mul_component = (\n",
    "        pipeline_pca_coeffs[index]\n",
    "        * pipeline_estimator_coeffs[index]\n",
    "        #         * mean_X_featurs\n",
    "    )\n",
    "    multiplied_components.append(mul_component)\n",
    "\n",
    "multiplied_components = np.array(multiplied_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape components to recover *channels x bins* structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplied_components = multiplied_components.reshape(\n",
    "    multiplied_components.shape[0], 2, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum data on components from feature extraction to get averaged time-features along the components.\n",
    "Since the components from feature extraction are weighted with the estimator coefficients, the data can be summed for the average results.\n",
    "\n",
    "Data will be *spatial_filter x bins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.negative(np.sum(multiplied_components, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of relationship between signal amplitudes and rumination.\n",
    "\n",
    "Blue color indicates that larger negative amplitude is associated with higher rumination level. Red color indicates that larger positive amplitude is associated with higher rumination level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# for ERP visualization\n",
    "yticklabels_ERP = red_box\n",
    "\n",
    "# for spatial_filter visualization\n",
    "yticklabels_SF = data.shape[0]\n",
    "\n",
    "sns.set(font_scale=3)\n",
    "plt.figure(figsize=(22, 16))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    data=data,\n",
    "    center=0,\n",
    "    cmap=\"vlag\",\n",
    "    yticklabels=yticklabels_SF,\n",
    "    xticklabels=[\n",
    "        -100,\n",
    "        -50,\n",
    "        0,\n",
    "        50,\n",
    "        100,\n",
    "        150,\n",
    "        200,\n",
    "        250,\n",
    "        300,\n",
    "        350,\n",
    "        400,\n",
    "        450,\n",
    "        500,\n",
    "        550,\n",
    "        600,\n",
    "    ],\n",
    ")\n",
    "fig = ax.get_figure()\n",
    "\n",
    "fig_name = \"ERP_svc_lin\"\n",
    "\n",
    "fig.savefig(\"rumination_classification_features_coeffs_\" + fig_name + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of signal at spatial filter components / channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "\n",
    "pipeline_without_feature_extraction = estimator.steps[:-3]\n",
    "features = Pipeline(pipeline_without_feature_extraction).transform(X_train)\n",
    "\n",
    "spatial_filter_num_components = int(\n",
    "    features.shape[-1] / int(X_train.shape[-1] / step_tp)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data on high/low classes for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high = []\n",
    "X_low = []\n",
    "\n",
    "for i in range(len(features)):\n",
    "    if y_train[i] == HIGH:\n",
    "        #         print(f\" IN HIGH: {y_train[i]}\")\n",
    "        X_high.append(features[i])\n",
    "    else:\n",
    "        X_low.append(features[i])\n",
    "#         print(f\" IN LOW: {y_train[i]}\")\n",
    "X_high = np.array(X_high)\n",
    "X_low = np.array(X_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_X_low = np.mean(X_low, axis=0)\n",
    "mean_X_low = mean_X_low.reshape(spatial_filter_num_components, -1)\n",
    "\n",
    "mean_X_high = np.mean(X_high, axis=0)\n",
    "mean_X_high = mean_X_high.reshape(spatial_filter_num_components, -1)\n",
    "\n",
    "mean_features = np.mean(features, axis=0)\n",
    "mean_features = mean_features.reshape(spatial_filter_num_components, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of signal on components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "components = [\"PCA 1\", \"PCA 2\"]\n",
    "bucket_width_ms = 1 / 256 * 12 * 1000\n",
    "xs = np.array([(bucket_width_ms * x - 100) for x in range(len(mean_X_low[0]))])\n",
    "for index in range(len(mean_X_low)):\n",
    "    for subindex in range(len(mean_X_low[index])):\n",
    "        data.append(\n",
    "            {\n",
    "                \"x\": xs[subindex],\n",
    "                \"y\": np.negative(mean_X_low[index][subindex]),\n",
    "                \"Rumination\": \"low\",\n",
    "                \"component\": components[index],\n",
    "            }\n",
    "        )\n",
    "        data.append(\n",
    "            {\n",
    "                \"x\": xs[subindex],\n",
    "                \"y\": np.negative(mean_X_high[index][subindex]),\n",
    "                \"Rumination\": \"high\",\n",
    "                \"component\": components[index],\n",
    "            }\n",
    "        )\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "g = sns.relplot(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    hue=\"Rumination\",\n",
    "    col=\"component\",\n",
    "    kind=\"line\",\n",
    "    data=df,\n",
    "    linewidth=5,\n",
    "    col_wrap=2,\n",
    "    zorder=0,\n",
    "    height=15,\n",
    "    aspect=2,\n",
    ")\n",
    "\n",
    "g.map(\n",
    "    plt.axhline, y=0, color=\".7\", dashes=(5, 5), zorder=0, linewidth=5\n",
    ").set_axis_labels(\"Time [ms]\", \"Amplitude [10*µV]\").set_titles(\"Component: {col_name}\")\n",
    "\n",
    "g.savefig(\"rumination_classification_components_signal_PCA_4_svc.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
