{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_transparent(x,y,col,data,ci='sd'):\n",
    "    sns.set(font_scale=2)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    col_wrap = None if col==None else 5\n",
    "    \n",
    "    hue_categories = ['lr_en not significant','lr_en', 'svc_lin not significant', 'svc_lin', 'svc_rbf not significant', 'svc_rbf']\n",
    "    hue_categories_colors = colors\n",
    "\n",
    "    data = data.reset_index()\n",
    "    hue_column = \"statistical significance \"\n",
    "    data[hue_column] = data.apply(lambda row: row[x] if row[\"p-value\"] < 0.05 else row[x]+\" not significant\", axis=1)\n",
    "    \n",
    "    palette = dict(zip(hue_categories, hue_categories_colors))\n",
    "    ax = sns.catplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        col=col,\n",
    "        col_order=['ICA_15', 'ICA_4', 'PCA_15', 'PCA_4', 'ERP'],\n",
    "        hue=hue_column,\n",
    "        ci=ci,\n",
    "        data=data, \n",
    "        kind='bar', \n",
    "        capsize=.05,\n",
    "        errwidth = 1,\n",
    "        legend=False,\n",
    "        col_wrap=col_wrap,\n",
    "        dodge=False,\n",
    "        palette=palette,\n",
    "        margin_titles=True,\n",
    "        sharex=True, \n",
    "        sharey=True,\n",
    "        height=5, \n",
    "        aspect=1,\n",
    "    ).set_titles(\"Pipeline: {col_name}\").set_axis_labels(\"\", \"Internal accuracy\").set_xticklabels([\"SVC-rbf\", \"SVC-lin\", \"LR-en\"])\n",
    "    \n",
    "    ax.savefig(\"rumination_classification_summary.png\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"../../data/results_classification/classification_results_without_functions_visualization_error.pkl\"\n",
    "results_df = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['pipeline_name'] = results_df['pipeline_name'].apply(lambda x: x[:-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting error bars - ugly hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates=10000\n",
    "\n",
    "#duplicate observations to get good std bars\n",
    "dfCopy = results_df.loc[results_df.index.repeat(duplicates)].copy()\n",
    "dfCopy['mean_cv_balanced_accuracy'] = np.random.normal(dfCopy['mean_cv_balanced_accuracy'].values,dfCopy['std_cv_balanced_accuracy'].values)\n",
    "dfCopy['mean_cv_precision'] = np.random.normal(dfCopy['mean_cv_precision'].values,dfCopy['std_cv_precision'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_transparent(x=\"model\", y=\"mean_cv_balanced_accuracy\", col=\"pipeline_name\", ci='sd', data=dfCopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add info about *precision* metric from cv_results - not included by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_precision_list = []\n",
    "for index, row in results_df.iterrows():\n",
    "    cv_results_params = row.cv_results['params']\n",
    "    params = row.parameters\n",
    "        \n",
    "    index = cv_results_params.index(params)\n",
    "    mean_train_precision = row.cv_results['mean_train_precision'][index]\n",
    "    mean_train_precision_list.append(mean_train_precision)\n",
    "\n",
    "mean_train_precision_list = np.array(mean_train_precision_list)\n",
    "mean_train_precision_df = pd.DataFrame(mean_train_precision_list)\n",
    "results_df['mean_train_precision'] = mean_train_precision_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw summary of all results - most important metrics and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_df = results_df[results_df['p-value'] <= 0.05]\n",
    "summarize_df = results_df\n",
    "summarize_df = summarize_df[[\"data_set\", \"mean_cv_balanced_accuracy\", \"mean_train_balanced_accuracy\", \"mean_cv_precision\", \"mean_train_precision\", \"pipeline_name\", \"model\", \"std_cv_balanced_accuracy\", \"std_cv_precision\", \"p-value\", 'parameters', 'external_test_accuracy']]\n",
    "summarize_df = summarize_df.reset_index()\n",
    "columns_order = [\"data_set\", \"pipeline_name\",  \"model\", \"mean_train_balanced_accuracy\", \"mean_cv_balanced_accuracy\", \"std_cv_balanced_accuracy\", \"mean_train_precision\", \"mean_cv_precision\", \"std_cv_precision\", \"p-value\", 'external_test_accuracy', \"parameters\"]\n",
    "summarize_df = summarize_df[columns_order].rename(columns = {'mean_train_balanced_accuracy': 'mean train accuracy', 'mean_cv_balanced_accuracy': 'mean test accuracy', \"std_cv_balanced_accuracy\": \"accuracy std\", \"mean_train_precision\" : \"mean train precision\", \"mean_cv_precision\": \"mean test precision\",  \"std_cv_precision\": \"precision std\", 'external_test_accuracy' : 'External validation accuracy'}, inplace = False)\n",
    "\n",
    "summarize_df['# spatial filter components'] = summarize_df['parameters'].apply(lambda x: x['ica__n_components'] if x.get('ica__n_components') is not None else (x['spatial_filter__n_components'] if x.get('spatial_filter__n_components') is not None else '-' ))\n",
    "summarize_df['# selected features'] = summarize_df['parameters'].apply(lambda x: x['feature_selection__n_components'])\n",
    "summarize_df = summarize_df.drop(columns=['parameters'])\n",
    "\n",
    "summarize_df[\"p-value\"] = pd.to_numeric(summarize_df[\"p-value\"])\n",
    "summarize_df = summarize_df.round({'mean train accuracy': 3, 'mean test accuracy': 3, 'accuracy std': 3, 'mean train precision': 3, 'mean test precision': 3, 'precision std': 3, 'p-value': 3, 'External validation accuracy' : 3,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results summary to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_df.to_csv(\n",
    "    \"classification_summary.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize permutation tests results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_scores = results_df['permutation_scores'].to_numpy()\n",
    "scores = results_df['mean_cv_balanced_accuracy'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = permutation_scores[10]\n",
    "test_score = scores[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.displot(test, bins=20, height=8.27, aspect=11.7/8.27)\n",
    "ax.set(xlabel='Accuracy score', ylabel='Count')\n",
    "\n",
    "score_label = (f\"Score on original\\ndata: {test_score:.3f}\")\n",
    "\n",
    "plt.text(0.649, 122, score_label, fontsize=12, bbox=dict(facecolor='blue', alpha=0.1, edgecolor='black', linewidth=2))\n",
    "plt.axvline(test_score, ls='--', color='r')\n",
    "\n",
    "ax.savefig(\"probalility_distribution_PCA_15_svc_lin.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
